{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install d2l","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-06T10:07:14.383839Z","iopub.execute_input":"2023-11-06T10:07:14.384206Z","iopub.status.idle":"2023-11-06T10:07:47.154348Z","shell.execute_reply.started":"2023-11-06T10:07:14.384176Z","shell.execute_reply":"2023-11-06T10:07:47.153290Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install torchinfo","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:08:33.464686Z","iopub.execute_input":"2023-11-06T10:08:33.465064Z","iopub.status.idle":"2023-11-06T10:08:44.797634Z","shell.execute_reply.started":"2023-11-06T10:08:33.465030Z","shell.execute_reply":"2023-11-06T10:08:44.796463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom d2l import torch as d2l","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:08:59.284863Z","iopub.execute_input":"2023-11-06T10:08:59.285219Z","iopub.status.idle":"2023-11-06T10:09:03.188500Z","shell.execute_reply.started":"2023-11-06T10:08:59.285186Z","shell.execute_reply":"2023-11-06T10:09:03.187214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data = pd.read_csv('/kaggle/input/weiboedit/weibo_xiugaishuju.csv', names=['label','review'], header=None)\nraw_data","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:09:04.812326Z","iopub.execute_input":"2023-11-06T10:09:04.812831Z","iopub.status.idle":"2023-11-06T10:09:05.046874Z","shell.execute_reply.started":"2023-11-06T10:09:04.812802Z","shell.execute_reply":"2023-11-06T10:09:05.045985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nraw_data","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:09:07.530609Z","iopub.execute_input":"2023-11-06T10:09:07.531611Z","iopub.status.idle":"2023-11-06T10:09:07.546010Z","shell.execute_reply.started":"2023-11-06T10:09:07.531564Z","shell.execute_reply":"2023-11-06T10:09:07.544962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\nfor train_index,test_index in split.split(raw_data['review'],raw_data['label']):\n    train_set = raw_data.iloc[train_index, :]\n    test_set = raw_data.iloc[test_index, :]\ntrain_set\ntest_set","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:09:10.173803Z","iopub.execute_input":"2023-11-06T10:09:10.174172Z","iopub.status.idle":"2023-11-06T10:09:10.698672Z","shell.execute_reply.started":"2023-11-06T10:09:10.174141Z","shell.execute_reply":"2023-11-06T10:09:10.697728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#使用过采样\nfrom imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=42)\ntrain_set_balanced_over, _ = ros.fit_resample(train_set, train_set['label'])\ntrain_set_balanced_over\n#使用欠采样\nfrom imblearn.under_sampling import RandomUnderSampler\nrus = RandomUnderSampler(random_state=42)\ntrain_set_balanced_under, _ = rus.fit_resample(train_set, train_set['label'])\ntrain_set_balanced_under","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:09:13.486703Z","iopub.execute_input":"2023-11-06T10:09:13.487071Z","iopub.status.idle":"2023-11-06T10:09:13.999009Z","shell.execute_reply.started":"2023-11-06T10:09:13.487042Z","shell.execute_reply":"2023-11-06T10:09:13.998108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nimport torchvision\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils import data\nfrom torchvision import transforms\nfrom torchinfo import summary\nimport collections\nfrom transformers import BertTokenizer, DataCollatorWithPadding","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:43:29.362087Z","iopub.execute_input":"2023-11-06T10:43:29.362785Z","iopub.status.idle":"2023-11-06T10:43:29.368538Z","shell.execute_reply.started":"2023-11-06T10:43:29.362752Z","shell.execute_reply":"2023-11-06T10:43:29.367555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32#超参数，批量大小\nmax_length = 64#超参数，最大长度\ntokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')#加载分词器","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:43:44.900758Z","iopub.execute_input":"2023-11-06T10:43:44.901557Z","iopub.status.idle":"2023-11-06T10:43:45.875200Z","shell.execute_reply.started":"2023-11-06T10:43:44.901523Z","shell.execute_reply":"2023-11-06T10:43:45.874498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cuda = True\ndevice = \"cuda\" if cuda else cpu","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:43:52.155205Z","iopub.execute_input":"2023-11-06T10:43:52.156061Z","iopub.status.idle":"2023-11-06T10:43:52.160476Z","shell.execute_reply.started":"2023-11-06T10:43:52.156025Z","shell.execute_reply":"2023-11-06T10:43:52.159360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data_loaders(data, tokenizer, batch_size, max_length, shuffle = True):#shuffle：需不需要打乱\n    collact = DataCollatorWithPadding(tokenizer)\n    dataset = []\n    texts = data['review'].to_list()#将评论转化列表\n    labels = data['label'].to_list()#将分类列表转化为列表\n    for i in tqdm(range(len(texts))):\n        text, label = texts[i], labels[i]\n        inputs = tokenizer(text = text, max_length = max_length, padding = 'max_length', truncation = True)#truncation是判断超过长度是否截断\n        inputs[\"labels\"] = label\n        dataset.append(inputs)\n    data_loader = DataLoader(dataset, batch_size = batch_size, shuffle = shuffle, collate_fn = collact)\n    return data_loader\n\ntrain_loader = get_data_loaders(train_set, tokenizer, batch_size = batch_size, max_length = max_length)#如果不使用采样操作，则第一个参数为：train_set；如果进行采样的话，则第一个输入为：train_set_balanced\ntest_loader = get_data_loaders(test_set, tokenizer, batch_size * 2, max_length = max_length, shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:43:56.895229Z","iopub.execute_input":"2023-11-06T10:43:56.895609Z","iopub.status.idle":"2023-11-06T10:44:30.712328Z","shell.execute_reply.started":"2023-11-06T10:43:56.895577Z","shell.execute_reply":"2023-11-06T10:44:30.711415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:45:01.441314Z","iopub.execute_input":"2023-11-06T10:45:01.441717Z","iopub.status.idle":"2023-11-06T10:45:02.558100Z","shell.execute_reply.started":"2023-11-06T10:45:01.441688Z","shell.execute_reply":"2023-11-06T10:45:02.557010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Bert_wwm_ext_ClassificationModel(nn.Module):\n    def __init__(self, hidden_size, num_classes):\n        super(Bert_wwm_ext_ClassificationModel, self).__init__()\n        self.pretrained_bert = BertForSequenceClassification.from_pretrained('hfl/chinese-bert-wwm-ext', num_labels=num_classes).to(device)\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.pretrained_bert(\n                                      input_ids = input_ids,\n                                      attention_mask = attention_mask,\n                                      token_type_ids = token_type_ids\n                                     )#需要返回input_ids;attention_mask;token_type_ids\n        logits = outputs.logits#直接拿出outputs中的最后一层\n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:59:31.346940Z","iopub.execute_input":"2023-11-06T10:59:31.347708Z","iopub.status.idle":"2023-11-06T10:59:31.354145Z","shell.execute_reply.started":"2023-11-06T10:59:31.347674Z","shell.execute_reply":"2023-11-06T10:59:31.352899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_wwm_ext_model = Bert_wwm_ext_ClassificationModel(hidden_size = 768, num_classes = 2).to(device)\n#输出模型参数\nsummary(bert_wwm_ext_model)\ntotal = sum ([param.nelement () for param in bert_wwm_ext_model.parameters ()]) \nprint (\"Number of parameters: %.2fM\" % (total/1e6))","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:59:33.524152Z","iopub.execute_input":"2023-11-06T10:59:33.524509Z","iopub.status.idle":"2023-11-06T10:59:35.135511Z","shell.execute_reply.started":"2023-11-06T10:59:33.524482Z","shell.execute_reply":"2023-11-06T10:59:35.134513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:59:38.164082Z","iopub.execute_input":"2023-11-06T10:59:38.165044Z","iopub.status.idle":"2023-11-06T10:59:38.169213Z","shell.execute_reply.started":"2023-11-06T10:59:38.165013Z","shell.execute_reply":"2023-11-06T10:59:38.168293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_wwm_ext_evaluate_model(net, test_iter, criterion, device = None):\n    \"\"\"\n    net: bert_model\n    data_loader: test_dataloader\n    criterion: CEloss;KLloss\n    device: GPU\n    \"\"\"\n    if isinstance(net, nn.Module):\n        net.eval()\n        if not device:\n            device = next(iter(net.parameters())).device\n    #在测试集上不需要梯度更新，所以这里直接梯度冻结        \n    with torch.no_grad():\n        test_y_all, prediction_all = [], []#用来存储测试集上的真实值和预测值\n        total_loss = 0.0#初始化损失值\n        for test_data in test_iter:\n            test_data = test_data.to(device)#将测试集数据挪到GPU上\n            out = net(\n                     input_ids = test_data[\"input_ids\"],\n                     attention_mask = test_data[\"attention_mask\"],\n                     token_type_ids = test_data[\"token_type_ids\"]\n                     )\n            test_y = test_data[\"labels\"]\n            \n            l = criterion(out, test_y)\n            total_loss += l.sum().item()#计算每个样本的损失\n            \n            prediction = out.argmax(dim = 1)\n            prediction = prediction.cpu().detach().numpy()#将测试集上的预测值从GPU上抽出并转化为list\n            test_y = test_y.cpu().detach().numpy()#将测试集上的真实值从GPU上抽出并转化为list\n            \n            test_y_all.append(test_y)#将真实值进行拼接\n            prediction_all.append(prediction)#将预测值进行拼接\n        \n        test_y_all = np.concatenate(test_y_all, axis=0)#将测试集的真实值按行进行拼接\n        prediction_all = np.concatenate(prediction_all, axis=0)#将测试集的预测值按行进行拼接\n        \n        #设置评价指标\n        acc_score = accuracy_score(test_y_all, prediction_all)#准确率\n        pre_score = precision_score(test_y_all, prediction_all, average = \"binary\")#二分类的精确率\n        rec_score = recall_score(test_y_all, prediction_all, average = \"binary\")#二分类的召回率\n        fscore = f1_score(test_y_all, prediction_all, average = \"binary\")#二分类的f1得分\n        \n        #返回测试集上模型的平均损失\n        avg_loss = total_loss / len(test_iter)\n        \n    #依次返回了acc;pre;rec,f1和loss    \n    return acc_score, pre_score, rec_score, fscore, avg_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:59:41.390059Z","iopub.execute_input":"2023-11-06T10:59:41.390455Z","iopub.status.idle":"2023-11-06T10:59:41.402914Z","shell.execute_reply.started":"2023-11-06T10:59:41.390422Z","shell.execute_reply":"2023-11-06T10:59:41.401920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"teacher_loss = nn.CrossEntropyLoss()\ntest_acc, test_pre, test_rec, test_f1, test_loss = bert_wwm_ext_evaluate_model(bert_wwm_ext_model, test_loader, criterion = teacher_loss)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:59:43.347846Z","iopub.execute_input":"2023-11-06T10:59:43.348916Z","iopub.status.idle":"2023-11-06T10:59:53.976587Z","shell.execute_reply.started":"2023-11-06T10:59:43.348867Z","shell.execute_reply":"2023-11-06T10:59:53.975779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time#输出单条数据的推理时间","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:59:57.840724Z","iopub.execute_input":"2023-11-06T10:59:57.841728Z","iopub.status.idle":"2023-11-06T10:59:57.846284Z","shell.execute_reply.started":"2023-11-06T10:59:57.841682Z","shell.execute_reply":"2023-11-06T10:59:57.845254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bert_wwm_ext_model_classification(net, train_iter, criterion, optimizer, num_epochs, device):\n    \"\"\"\n    net: bert_model\n    data_loader: train_dataloader\n    loss: bert_teacher_loss\n    optimizer: bert_teacher_optimizer\n    num_epochs: epoch\n    device: GPU\n    \"\"\"\n    net.train()\n    for epoch in range(num_epochs):\n        timer = d2l.Timer()#设置模型训练时间\n        train_y_all, prediction_all = [], []#用来存储训练数据的真实值和预测值\n        for train_data in tqdm(train_iter):\n            train_data = train_data.to(device)#将训练数据集挪到GPU上\n            \n            start_time = time.time()#计时开始\n            \n            out = net(\n                     input_ids = train_data[\"input_ids\"],\n                     attention_mask = train_data[\"attention_mask\"],\n                     token_type_ids = train_data[\"token_type_ids\"]\n                     )\n            train_y = train_data[\"labels\"]\n            \n            #计算损失\n            loss = criterion(out, train_y)\n            \n            #梯度清除\n            optimizer.zero_grad()\n            #反向传播\n            loss.backward()\n            #梯度更新\n            optimizer.step()\n            \n            prediction = out.argmax(dim = 1)#将训练集上的预测值从logti——>softmax\n            \n            end_time = time.time()\n            inference_time = (end_time - start_time) * 1000#计算单条推理时间，并以毫秒进行显示\n            \n            #将prediction和train_y从GPU中抽到CPU\n            prediction = prediction.cpu().detach()\n            train_y = train_y.cpu().detach()\n            \n            train_y_all.append(train_y)#存储当前epoch中的真实标签\n            prediction_all.append(prediction)#存储当前epoch中的测试值\n        \n        train_y_all = torch.cat(train_y_all, dim = 0)#按行链接所有epoch的真实标签\n        prediction_all = torch.cat(prediction_all, dim = 0)#按行链接所有epoch的预测值\n        #设置模型评价指标    \n        acc_score = accuracy_score(train_y_all,  prediction_all)\n        pre_score = precision_score(train_y_all, prediction_all, average = \"binary\")\n        rec_score = recall_score(train_y_all, prediction_all, average = \"binary\")\n        fscore = f1_score(train_y_all, prediction_all, average = \"binary\")\n        \n        #在训练集上进行模型训练，并输出评价指标和loss    \n        print(f'epoch{epoch+1}, train_loss {loss:.4f}, train_acc {acc_score:.4f}, train_pre {pre_score:.4f}, train_rec {rec_score:.4f}, train_f1 {fscore:.4f}')\n        \n        #在测试集上进行模型评估，并输出评价指标和loss\n        test_acc, test_pre, test_rec, test_f1, test_loss = bert_evaluate_model(bert_model, test_loader, criterion = teacher_loss)\n        print(f'test loss {test_loss:.4f}', f'test acc {test_acc:.4f}', f'test pre {test_pre:.4f}', f'test rec {test_rec:.4f}', f'test f1 {test_f1:.4f}')\n        \n        #计算模型推理时间；计算出单条数据的推理时间\n        print(\"Total time : {:.2f}\".format(timer.stop()), \"Single_data time: {:.2f} ms\".format(inference_time))","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:59:59.971014Z","iopub.execute_input":"2023-11-06T10:59:59.971492Z","iopub.status.idle":"2023-11-06T10:59:59.989587Z","shell.execute_reply.started":"2023-11-06T10:59:59.971451Z","shell.execute_reply":"2023-11-06T10:59:59.987441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 3e-5#学习率：超参数\nnum_epochs = 3#迭代周期：超参数","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:00:02.706247Z","iopub.execute_input":"2023-11-06T11:00:02.706625Z","iopub.status.idle":"2023-11-06T11:00:02.711280Z","shell.execute_reply.started":"2023-11-06T11:00:02.706595Z","shell.execute_reply":"2023-11-06T11:00:02.710199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_wwm_ext_model = Bert_wwm_ext_ClassificationModel(hidden_size = 768, num_classes = 2).to(device)\nteacher_loss = nn.CrossEntropyLoss()\nteacher_optimizer = AdamW(bert_wwm_ext_model.parameters(), lr=lr)#优化器的选择：超参数","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:00:04.805989Z","iopub.execute_input":"2023-11-06T11:00:04.806391Z","iopub.status.idle":"2023-11-06T11:00:06.284572Z","shell.execute_reply.started":"2023-11-06T11:00:04.806346Z","shell.execute_reply":"2023-11-06T11:00:06.283785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_wwm_ext_model_classification(net = bert_wwm_ext_model, train_iter = train_loader, criterion = teacher_loss, optimizer = teacher_optimizer, num_epochs = num_epochs, device = device)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T10:48:18.865683Z","iopub.execute_input":"2023-11-06T10:48:18.866005Z","iopub.status.idle":"2023-11-06T10:56:02.290213Z","shell.execute_reply.started":"2023-11-06T10:48:18.865977Z","shell.execute_reply":"2023-11-06T10:56:02.289156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 保存模型\ntorch.save(bert_wwm_ext_model.state_dict(), '/kaggle/working/bert_wwm_ext_model.pth')#注意保存路径。其中，第一个bert_model的意思是自己定义的模型；第二个bert_model的意思是，保存预训练好的模型的名字","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:00:08.746455Z","iopub.execute_input":"2023-11-06T11:00:08.747083Z","iopub.status.idle":"2023-11-06T11:00:09.252531Z","shell.execute_reply.started":"2023-11-06T11:00:08.747053Z","shell.execute_reply":"2023-11-06T11:00:09.251279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 加载保存的BERT模型参数\nbert_model = BertClassificationModel(hidden_size=768, num_classes=2)\nbert_wwm_ext_model.load_state_dict(torch.load('/kaggle/working/bert_wwm_ext_model.pth'))#注意加载路径。其中，第一个bert_model的意思是自己定义的模型；第二个bert_model的意思是，保存预训练好的模型的名字","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:00:22.734103Z","iopub.execute_input":"2023-11-06T11:00:22.734940Z","iopub.status.idle":"2023-11-06T11:00:24.451666Z","shell.execute_reply.started":"2023-11-06T11:00:22.734909Z","shell.execute_reply":"2023-11-06T11:00:24.450620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BiLSTMClassification(nn.Module):\n    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, **kwargs):\n        \"\"\"\n        vocab_size：词汇表的大小\n        embed_size：嵌入层大小embedding_size\n        hiddern_size：隐藏层大小\n        num_layers：层数\n        \"\"\"\n        super(BiLSTMClassification, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.encoder = nn.LSTM(embed_size, hidden_size, num_layers = num_layers, bidirectional = True, dropout = 0.5)\n        self.decoder = nn.Sequential(\n                                      nn.Linear(2 * hidden_size, 2)#双向LSTM中，先前向一次再后向一次，所以是*2\n                                      )\n    def forward(self, inputs):\n        \"\"\"\n        input_shape: (batch_size, max_length)\n        \"\"\"\n        embeddings = self.embedding(inputs).transpose(0, 1)#shape：(max_length, batch_size, embed_size)#保证max_length在第一个维度\n        outputs, (_,_) = self.encoder(embeddings)#输出为: (outputs, hidden_state)，只取出outputs所对应的向量，舍去hidden_state所对应的向量\n        outputs_fw = outputs[:, :, :hidden_size]#BiLSTM需要拿出前向LSMT的最后一个时间步骤的向量\n        outputs_bw = outputs[:, :, hidden_size:]#BiLSTM需要拿出后向LSTM的第一个时间步骤的向量\n        logits = self.decoder(torch.cat((outputs_fw[-1], outputs_bw[0]), dim = 1))#将最后一个时间步骤的向量和第一个时间步骤的向量进行concat操作\n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:02:01.492858Z","iopub.execute_input":"2023-11-06T11:02:01.493685Z","iopub.status.idle":"2023-11-06T11:02:01.503040Z","shell.execute_reply.started":"2023-11-06T11:02:01.493649Z","shell.execute_reply":"2023-11-06T11:02:01.501988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = tokenizer.get_vocab()\nlen(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:02:07.068843Z","iopub.execute_input":"2023-11-06T11:02:07.069181Z","iopub.status.idle":"2023-11-06T11:02:07.079453Z","shell.execute_reply.started":"2023-11-06T11:02:07.069155Z","shell.execute_reply":"2023-11-06T11:02:07.078437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#定义学生模型参数\nembed_size, hidden_size, num_layers = 64, 64, 2\nbilstm_model = BiLSTMClassification(len(vocab_size), embed_size, hidden_size, num_layers).to(device)#vocab_szie = tokenizer.get_vocab()\nsummary(bilstm_model)\ntotal = sum([param.nelement() for param in bilstm_model.parameters()])\nprint(\"Number of parameters: %.2fM\" % (total/1e6))","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:02:10.235681Z","iopub.execute_input":"2023-11-06T11:02:10.236408Z","iopub.status.idle":"2023-11-06T11:02:10.582259Z","shell.execute_reply.started":"2023-11-06T11:02:10.236353Z","shell.execute_reply":"2023-11-06T11:02:10.581289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bilstm_evaluate_model(net, test_iter, criterion, device = None):\n    \"\"\"\n    net: bert_model\n    data_loader: test_dataloader\n    criterion: CEloss;KLloss\n    device: GPU\n    \"\"\"\n    if isinstance(net, nn.Module):\n        net.eval()\n        if not device:\n            device = next(iter(net.parameters())).device\n    #在测试集上不需要梯度更新，所以这里直接梯度冻结        \n    with torch.no_grad():\n        test_y_all, prediction_all = [], []#用来存储测试集上的真实值和预测值\n        total_loss = 0.0#初始化损失值\n        for test_data in test_iter:\n            test_data = test_data.to(device)#将测试集数据挪到GPU上\n            out = net(\n                     inputs = test_data[\"input_ids\"]\n                     )\n            test_y = test_data[\"labels\"]\n            \n            l = criterion(out, test_y)\n            total_loss += l.sum().item()#计算每个样本的损失\n            \n            prediction = out.argmax(dim = 1)\n            prediction = prediction.cpu().detach().numpy()#将测试集上的预测值从GPU上抽出并转化为list\n            test_y = test_y.cpu().detach().numpy()#将测试集上的真实值从GPU上抽出并转化为list\n            \n            test_y_all.append(test_y)#将真实值进行拼接\n            prediction_all.append(prediction)#将预测值进行拼接\n        \n        test_y_all = np.concatenate(test_y_all, axis=0)#将测试集的真实值按行进行拼接\n        prediction_all = np.concatenate(prediction_all, axis=0)#将测试集的预测值按行进行拼接\n        \n        #设置评价指标\n        acc_score = accuracy_score(test_y_all, prediction_all)#准确率\n        pre_score = precision_score(test_y_all, prediction_all, average = \"binary\")#二分类的精确率\n        rec_score = recall_score(test_y_all, prediction_all, average = \"binary\")#二分类的召回率\n        fscore = f1_score(test_y_all, prediction_all, average = \"binary\")#二分类的f1得分\n        \n        #返回测试集上模型的平均损失\n        avg_loss = total_loss / len(test_iter)\n        \n    #依次返回了acc;pre;rec,f1和loss    \n    return acc_score, pre_score, rec_score, fscore, avg_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:02:24.224301Z","iopub.execute_input":"2023-11-06T11:02:24.225027Z","iopub.status.idle":"2023-11-06T11:02:24.235221Z","shell.execute_reply.started":"2023-11-06T11:02:24.224991Z","shell.execute_reply":"2023-11-06T11:02:24.234367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"student_loss = nn.CrossEntropyLoss()\ntest_acc, test_pre, test_rec, test_f1, test_loss = bilstm_evaluate_model(bilstm_model, test_loader, criterion = student_loss)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:02:26.485527Z","iopub.execute_input":"2023-11-06T11:02:26.486307Z","iopub.status.idle":"2023-11-06T11:02:27.263492Z","shell.execute_reply.started":"2023-11-06T11:02:26.486268Z","shell.execute_reply":"2023-11-06T11:02:27.262601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bilstmmodel_classification(net, train_iter, criterion, optimizer, num_epochs, device):\n    \"\"\"\n    net: bilstm_model,学生模型\n    train_iter: train_loader,数据集\n    criterion: bilstm_student_loss,学生模型损失函数\n    optimizer: bilstm_student_optimizer,学生模型优化器\n    num_epoch: student_epoch，学生模型迭代周期\n    device: GPU,加速器\n    \"\"\"\n    for epoch in range(num_epochs):\n        net.train()  # 设置模型为训练模式\n        timer = d2l.Timer()#设置模型训练时间\n        train_y_all, prediction_all = [], []#用来存储训练数据的真实值和预测值\n        for train_data in tqdm(train_iter):\n            train_data = train_data.to(device)#将训练数据集挪到GPU上\n            \n            start_time = time.time()#计时开始\n            \n            out = net(\n                     inputs = train_data[\"input_ids\"]#只需要去除tokenizer中的input_ids即可\n                     )\n            train_y = train_data[\"labels\"]\n            \n            #计算损失\n            loss = criterion(out, train_y)\n            \n            #梯度清除\n            optimizer.zero_grad()\n            #反向传播\n            loss.backward()\n            #梯度更新\n            optimizer.step()\n            \n            prediction = out.argmax(dim = 1)#将训练集上的预测值从logit——>softmax\n            \n            end_time = time.time()\n            inference_time = (end_time - start_time) * 1000#计算单条推理时间，并以毫秒进行显示\n            \n            #将prediction和train_y从GPU中抽到CPU\n            prediction = prediction.cpu().detach()\n            train_y = train_y.cpu().detach()\n            \n            train_y_all.append(train_y)#存储当前epoch中的真实标签\n            prediction_all.append(prediction)#存储当前epoch中的测试值\n        \n        train_y_all = torch.cat(train_y_all, dim = 0)#按行链接所有epoch的真实标签\n        prediction_all = torch.cat(prediction_all, dim = 0)#按行链接所有epoch的预测值\n        #设置模型评价指标    \n        acc_score = accuracy_score(train_y_all,  prediction_all)\n        pre_score = precision_score(train_y_all, prediction_all, average = \"binary\")\n        rec_score = recall_score(train_y_all, prediction_all, average = \"binary\")\n        fscore = f1_score(train_y_all, prediction_all, average = \"binary\")\n        \n        #在训练集上进行模型训练，并输出评价指标和loss    \n        print(f'epoch{epoch+1}, train_loss {loss:.4f}, train_acc {acc_score:.4f}, train_pre {pre_score:.4f}, train_rec {rec_score:.4f}, train_f1 {fscore:.4f}')\n        \n        #在测试集上进行模型评估，并输出评价指标和loss\n        test_acc, test_pre, test_rec, test_f1, test_loss = bilstm_evaluate_model(bilstm_model, test_loader, criterion = student_loss)\n        print(f'test loss {test_loss:.4f}', f'test acc {test_acc:.4f}', f'test pre {test_pre:.4f}', f'test rec {test_rec:.4f}', f'test f1 {test_f1:.4f}')\n        \n        #计算模型推理时间；计算出单条数据的推理时间\n        print(\"Total time : {:.2f}\".format(timer.stop()), \"Single_data time: {:.2f} ms\".format(inference_time))","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:02:32.170732Z","iopub.execute_input":"2023-11-06T11:02:32.171536Z","iopub.status.idle":"2023-11-06T11:02:32.184392Z","shell.execute_reply.started":"2023-11-06T11:02:32.171501Z","shell.execute_reply":"2023-11-06T11:02:32.183216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 0.0001#学习率：超参数\nnum_epochs = 3#迭代周期：超参数","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:02:35.915348Z","iopub.execute_input":"2023-11-06T11:02:35.915785Z","iopub.status.idle":"2023-11-06T11:02:35.920821Z","shell.execute_reply.started":"2023-11-06T11:02:35.915745Z","shell.execute_reply":"2023-11-06T11:02:35.919750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bilstm_model = BiLSTMClassification(len(vocab_size), embed_size, hidden_size, num_layers).to(device)#vocab_szie = tokenizer.get_vocab()\nstudent_loss = nn.CrossEntropyLoss()\nstudent_optimizer = AdamW(bilstm_model.parameters(), lr=lr)#优化器的选择：超参数","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:02:37.709661Z","iopub.execute_input":"2023-11-06T11:02:37.710007Z","iopub.status.idle":"2023-11-06T11:02:37.730940Z","shell.execute_reply.started":"2023-11-06T11:02:37.709982Z","shell.execute_reply":"2023-11-06T11:02:37.729907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bilstmmodel_classification(bilstm_model, train_loader, criterion = student_loss, optimizer = student_optimizer, num_epochs = num_epochs, device = device)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:02:39.453581Z","iopub.execute_input":"2023-11-06T11:02:39.453970Z","iopub.status.idle":"2023-11-06T11:02:54.707140Z","shell.execute_reply.started":"2023-11-06T11:02:39.453939Z","shell.execute_reply":"2023-11-06T11:02:54.706125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def kd_evaluate_model(teacher_net, student_net, test_iter, hard_loss, soft_loss, device = None):\n    \"\"\"\n    net: bert_model\n    data_loader: test_dataloader\n    criterion: CEloss;KLloss\n    device: GPU\n    \"\"\"\n    if isinstance(student_net, nn.Module):\n        teacher_net.eval()\n        student_net.eval()\n        if not device:\n            device = next(iter(student_net.parameters())).device\n    #在测试集上不需要梯度更新，所以这里直接梯度冻结        \n    with torch.no_grad():\n        test_y_all, student_prediction_all = [], []#用来存储测试集上的真实值和预测值\n        total_loss = 0.0#初始化损失值\n        for test_data in test_iter:\n            test_data = test_data.to(device)#将测试集数据挪到GPU上\n            teacher_out = teacher_net(\n                                     input_ids = test_data[\"input_ids\"],\n                                     attention_mask = test_data[\"attention_mask\"],\n                                     token_type_ids = test_data[\"token_type_ids\"]\n                                     )\n            student_out = student_net(\n                                     inputs = test_data[\"input_ids\"]\n                                     )\n            test_y = test_data[\"labels\"]\n            student_loss = hard_loss(student_out, test_y)\n            dist_loss = soft_loss(\n                                 F.log_softmax(student_out / T, dim = -1),\n                                 F.softmax(teacher_out / T, dim = -1)\n                                 )\n            total_loss = (1 - alpha) * student_loss + alpha * dist_loss\n            total_loss += total_loss.sum().item()#计算每个样本的损失\n            \n            student_prediction = student_out.argmax(dim = 1)\n            student_prediction = student_prediction.cpu().detach().numpy()#将测试集上的预测值从GPU上抽出并转化为list\n            test_y = test_y.cpu().detach().numpy()#将测试集上的真实值从GPU上抽出并转化为list\n            \n            test_y_all.append(test_y)#将真实值进行拼接\n            student_prediction_all.append(student_prediction)#将预测值进行拼接\n        \n        test_y_all = np.concatenate(test_y_all, axis=0)#将测试集的真实值按行进行拼接\n        student_prediction_all = np.concatenate(student_prediction_all, axis=0)#将测试集的预测值按行进行拼接\n        \n        #设置评价指标\n        acc_score = accuracy_score(test_y_all, student_prediction_all)#准确率\n        pre_score = precision_score(test_y_all, student_prediction_all, average = \"binary\")#二分类的精确率\n        rec_score = recall_score(test_y_all, student_prediction_all, average = \"binary\")#二分类的召回率\n        fscore = f1_score(test_y_all, student_prediction_all, average = \"binary\")#二分类的f1得分\n        \n        #返回测试集上模型的平均损失\n        avg_loss = total_loss / len(test_iter)\n        \n    #依次返回了acc;pre;rec,f1和loss    \n    return acc_score, pre_score, rec_score, fscore, avg_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:03:00.048726Z","iopub.execute_input":"2023-11-06T11:03:00.049483Z","iopub.status.idle":"2023-11-06T11:03:00.062131Z","shell.execute_reply.started":"2023-11-06T11:03:00.049443Z","shell.execute_reply":"2023-11-06T11:03:00.061163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kd_lr = 0.001\nkd_num_epochs = 3\nteacher_model = bert_wwm_ext_model.to(device)\nstudent_model = bilstm_model.to(device)\nkd_hard_loss = nn.CrossEntropyLoss()\nkd_soft_loss = nn.KLDivLoss(reduction = 'batchmean')\nkd_optimizer = torch.optim.Adam(student_model.parameters(), lr = kd_lr)#优化器的选择：超参数\nT, alpha = 2, 0.8","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:03:20.632303Z","iopub.execute_input":"2023-11-06T11:03:20.632675Z","iopub.status.idle":"2023-11-06T11:03:20.644540Z","shell.execute_reply.started":"2023-11-06T11:03:20.632647Z","shell.execute_reply":"2023-11-06T11:03:20.643649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_acc, test_pre, test_rec, test_f1, test_loss = kd_evaluate_model(teacher_net = bert_wwm_ext_model, student_net = student_model, test_iter = test_loader, hard_loss = kd_hard_loss, soft_loss = kd_soft_loss)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:03:30.291897Z","iopub.execute_input":"2023-11-06T11:03:30.292421Z","iopub.status.idle":"2023-11-06T11:03:41.110882Z","shell.execute_reply.started":"2023-11-06T11:03:30.292358Z","shell.execute_reply":"2023-11-06T11:03:41.110069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def kd_blk(teacher_net, student_net, train_iter, hard_loss, soft_loss, optimizer, T, alpha, num_epochs, device):\n    \"\"\"\n    teacher_net: 教师模型\n    student_net: 学生模型\n    train_iter: 训练集迭代器\n    hard_loss, soft_loss: 硬损失，软损失\n    T, alpha, num_epochs: 蒸馏温度， 蒸馏系数， 迭代次数\n    \"\"\"\n    student_net = nn.DataParallel(student_net).to(device)#使用多GPU进行并行训练教师模型\n    teacher_net = nn.DataParallel(teacher_net).to(device)#使用多GPU进行并行训练学生模型\n    teacher_net.eval()#蒸馏过程中，教师模型参数固定\n    for epoch in range(num_epochs):\n        timer = d2l.Timer()#设置模型参数训练时间\n        student_net.train()#设置学生模型为训练模型\n        train_y_all, student_prediction_all = [], []#用来存储训练数据的真实值和预测值\n        for train_data in tqdm(train_iter):\n            train_data = train_data.to(device)#将训练数据集挪到GPU上\n            \n            start_time = time.time()#计时开始\n            \n            #关闭教师模型梯度，使得教师模型不参与梯度更新\n            with torch.no_grad():\n                #输出教师模型的预测值\n                teacher_pre = teacher_net(\n                                         input_ids = train_data[\"input_ids\"],\n                                         attention_mask = train_data[\"attention_mask\"],\n                                         token_type_ids = train_data[\"token_type_ids\"]\n                                         )\n            #输出学生模型的预测值\n            student_pre = student_net(\n                                     inputs = train_data[\"input_ids\"]\n                                     )\n            #输出学生模型的预测值\n            student_pre = student_net(\n                                     inputs = train_data[\"input_ids\"]\n                                     )\n            #输出真实值\n            train_y = train_data[\"labels\"]\n            \n            #计算硬损失（学生损失）\n            student_loss = hard_loss(student_pre, train_y)\n            #计算软损失（蒸馏损失）\n            dist_loss = soft_loss(F.log_softmax(student_pre / T, dim = -1),\n                                  F.softmax(teacher_pre / T, dim = -1)\n                                 )\n            #计算总损失\n            total_loss = (1 - alpha) * student_loss + alpha * dist_loss\n            \n            #梯度清除\n            optimizer.zero_grad()\n            #反向传播\n            total_loss.backward()\n            #梯度更新\n            optimizer.step()\n            \n            #学生模型用来预测\n            student_prediction = student_pre.argmax(dim = 1)#将训练集上的预测值logit——>softmax\n            \n            end_time = time.time()\n            inference_time = (end_time - start_time) * 1000#计算单条推理时间，并以毫秒显示\n            \n            #将student_prediction和train_y从GPU中抽到CPU\n            student_prediction = student_prediction.cpu().detach()\n            train_y = train_y.cpu().detach()\n            \n            train_y_all.append(train_y)#存储当前epoch中的真实标签\n            student_prediction_all.append(student_prediction)#存储当前epoch中的学生模型的预测值\n            \n        train_y_all = torch.cat(train_y_all, dim = 0)#按行链接所有epoch的真实标签\n        student_prediction_all = torch.cat(student_prediction_all, dim = 0)#按行链接所有epoch的学生预测值\n        \n        #设置评价指标\n        acc_score = accuracy_score(train_y_all,  student_prediction_all)\n        pre_score = precision_score(train_y_all, student_prediction_all, average = \"binary\")\n        rec_score = recall_score(train_y_all, student_prediction_all, average = \"binary\")\n        fscore = f1_score(train_y_all, student_prediction_all, average = \"binary\")   \n        \n        #在训练集上进行模型训练，并输出评价指标和loss\n        print(f'epoch{epoch+1}, train_loss {total_loss:.4f}, train acc {acc_score:.4f}, train pre {pre_score:.4f}, train rec {rec_score:.4f}, train f1 {fscore:.4f}')\n        \n        #在测试集上进行模型评估，并输出评价指标和loss\n        test_acc, test_pre, test_rec, test_f1, test_loss = kd_evaluate_model(teacher_net = bert_model, student_net = student_model, test_iter = test_loader, hard_loss = kd_hard_loss, soft_loss = kd_soft_loss)\n        print(f'test loss {test_loss:.4f}', f'test acc {test_acc:.4f}', f'test pre {test_pre:.4f}', f'test rec {test_rec:.4f}', f'test f1 {test_f1:.4f}')\n        \n        #计算模型推理时间，计算出单条数据的推理时间\n        print(\"Total time : {:.2f}\".format(timer.stop()), \"Single_data time: {:.2f} ms\".format(inference_time))","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:03:46.109660Z","iopub.execute_input":"2023-11-06T11:03:46.110050Z","iopub.status.idle":"2023-11-06T11:03:46.126426Z","shell.execute_reply.started":"2023-11-06T11:03:46.110018Z","shell.execute_reply":"2023-11-06T11:03:46.125397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kd_lr = 0.001\nkd_num_epochs = 3","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:04:06.600410Z","iopub.execute_input":"2023-11-06T11:04:06.601151Z","iopub.status.idle":"2023-11-06T11:04:06.605753Z","shell.execute_reply.started":"2023-11-06T11:04:06.601119Z","shell.execute_reply":"2023-11-06T11:04:06.604539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"teacher_model = bert_wwm_ext_model.to(device)\nstudent_model = bilstm_model.to(device)\nkd_hard_loss = nn.CrossEntropyLoss()\nkd_soft_loss = nn.KLDivLoss(reduction = 'batchmean')\nkd_optimizer = torch.optim.Adam(student_model.parameters(), lr = kd_lr)#优化器的选择：超参数\nT, alpha = 2, 0.8","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:04:09.961586Z","iopub.execute_input":"2023-11-06T11:04:09.962854Z","iopub.status.idle":"2023-11-06T11:04:09.979795Z","shell.execute_reply.started":"2023-11-06T11:04:09.962806Z","shell.execute_reply":"2023-11-06T11:04:09.978147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kd_blk(teacher_net = teacher_model, student_net = student_model, train_iter = train_loader, hard_loss = kd_hard_loss, soft_loss = kd_soft_loss, optimizer = kd_optimizer, T = T, alpha = alpha, num_epochs = kd_num_epochs, device = device)","metadata":{"execution":{"iopub.status.busy":"2023-11-06T11:04:11.482230Z","iopub.execute_input":"2023-11-06T11:04:11.483041Z","iopub.status.idle":"2023-11-06T11:11:34.439518Z","shell.execute_reply.started":"2023-11-06T11:04:11.483006Z","shell.execute_reply":"2023-11-06T11:11:34.438459Z"},"trusted":true},"execution_count":null,"outputs":[]}]}